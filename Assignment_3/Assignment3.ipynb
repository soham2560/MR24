{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 : Extrinsic Estimation (20 Points)\n",
    "\n",
    "You are given 5 images in the `data/Images` folder, and their corresponding 2d-3d correspondences in `data/Correspondences`. Use this information with the Ceres Solver to optimize for the extrinsic of each image using Reprojection Loss.\n",
    "\n",
    "### Camera Intrinsics:\n",
    "`fx` = `fy` = 721.5\n",
    " \n",
    "`cx` and `cy` : $\\frac{width}{2}$ and $\\frac{height}{2}$\n",
    "\n",
    "### Format for Correspondences:\n",
    "`px py X Y Z i`\n",
    "where: `p_x` and `p_y` are the 2d image coordinates, and `X`, `Y` and `Z` are the coordinates of the 3d correspondence for the 2d coordinate, and `i` is the 3d point's index in the pointcloud (you can ignore i, just use the rest of the information as is).\n",
    "\n",
    "\n",
    "### Resources:\n",
    "http://ceres-solver.org/nnls_solving.html <br>\n",
    "http://ceres-solver.org/nnls_modeling.html <br>\n",
    "http://ceres-solver.org/nnls_tutorial.html <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 : Two-View Geometry Task (35 Points)\n",
    "\n",
    "### Note:  Use images in folder data_Q2  and the file `data_Q2/intrinsics.txt` has camera intrinsics \n",
    "### Don't use Opencv functions unless explicitly mentioned. \n",
    "\n",
    "<img src=\"data_Q2/first_img.jpg\" alt=\"image 1\" width=\"400\"/>\n",
    "<img src=\"data_Q2/second_img.jpg\" alt=\"image 2\" width=\"400\"/>\n",
    "\n",
    "### 1. Feature Extraction and Matching\n",
    "   **Implement a function to detect and match feature points between two images using OpenCV's SIFT or ORB.**  \n",
    "   Visualize the matched points and filter outliers using RANSAC. \n",
    "   \n",
    "   Theory : How do you ensure that the matches are accurate, and what thresholding techniques do you apply?\n",
    "\n",
    "### 2. Fundamental Matrix Estimation\n",
    "   **Write a function `compute_fundamental_matrix` to estimate the fundamental matrix using the 8-point algorithm.**  \n",
    "   Normalize the points before estimating and check the rank of the matrix. Explain why normalization is essential in this context.\n",
    "   \n",
    "   Extend this to include the 7-point algorithm, and compare the accuracy of the two methods on your matched points.\n",
    "\n",
    "### 3. Epipolar Lines Visualization\n",
    "   **Write a function to plot epipolar lines on both images given the estimated fundamental matrix.**  \n",
    "   For each matched feature in the first image, compute and draw the corresponding epipolar line in the second image.\n",
    "\n",
    "   Theory (not part of bonus) : What methods can you use to verify that the epipolar lines are accurate?\n",
    "\n",
    "### 4. Epipole Computation\n",
    "   **Implement a function to calculate the epipoles from the fundamental matrix.**  \n",
    "   Verify their correctness by showing that epipolar lines intersect at these points.\n",
    "\n",
    "   Theory : What happens to the epipole location if the fundamental matrix is rank-deficient, and how can you simulate this scenario?\n",
    "\n",
    "### 5. Camera Pose from Fundamental Matrix\n",
    "   **Write a function `camera_pose_from_fundamental` to compute the four possible camera poses (rotation `R` and translation `t`) from the fundamental matrix and intrinsic matrix `K`.**\n",
    "\n",
    "   Theory (not part of bonus) : Explain why there are four possible solutions and how to verify which is the correct one.\n",
    "\n",
    "### 6. Triangulation of 3D Points\n",
    "   **Implement linear triangulation in a function `linear_triangulation` that takes the projection matrices and point correspondences to compute 3D points.**\n",
    "\n",
    "   Visualize the triangulated points in 3D and describe how the accuracy of 3D points depends on the quality of the fundamental matrix and camera pose.\n",
    "\n",
    "### 7. Cheirality Condition\n",
    "   **Write a function `check_cheirality` to ensure that the triangulated 3D points satisfy the cheirality condition for a given camera pose.**\n",
    "\n",
    "   Use this function to select the correct camera pose among the four possibilities by maximizing the number of points that satisfy the cheirality condition.\n",
    "\n",
    "\n",
    "### 8. Fundamental Matrix Validation\n",
    "   **Write a function `validate_fundamental_matrix` to validate the estimated fundamental matrix by checking the epipolar constraint for a set of matched points.**  \n",
    "   Calculate the residual error for each point pair and analyze the overall error.\n",
    "\n",
    "   Theory : What threshold should you use to determine if a point satisfies the constraint?\n",
    "\n",
    "### 9. Camera Pose Visualization\n",
    "   **Use a 3D plotting library to visualize the computed camera poses and triangulated points in 3D space.**  \n",
    "   Write a function that takes in camera poses and 3D points and plots them.\n",
    "\n",
    "   Experiment with different camera poses and triangulated points to observe how pose selection affects the scene structure.\n",
    "\n",
    "### 10. Error Analysis for Two-view Geometry\n",
    "   **Implement a function to calculate and plot the reprojection error for triangulated 3D points.**  \n",
    "   Measure the difference between original 2D points and the reprojected points on both images.\n",
    "\n",
    "   Theory : How does this error analysis help in refining the camera pose selection?\n",
    "\n",
    "\n",
    "### Bonus : All theory questions are part of bonus unless mentioned explicitly. (15 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "def normalize_points(points):\n",
    "    mean = np.mean(points, axis=0)\n",
    "    std = np.std(points)\n",
    "    transform = np.array([\n",
    "        [1/std, 0, -mean[0]/std],\n",
    "        [0, 1/std, -mean[1]/std],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    homogeneous_points = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    normalized = (transform @ homogeneous_points.T).T\n",
    "    return normalized[:, :2], transform\n",
    "\n",
    "def compute_fundamental_matrix(points1, points2):\n",
    "    norm_points1, transform1 = normalize_points(points1)\n",
    "    norm_points2, transform2 = normalize_points(points2)\n",
    "\n",
    "    A = np.array([\n",
    "        [x2 * x1, x2 * y1, x2, y2 * x1, y2 * y1, y2, x1, y1, 1]\n",
    "        for (x1, y1), (x2, y2) in zip(norm_points1, norm_points2)\n",
    "    ])\n",
    "\n",
    "    _, _, Vh = np.linalg.svd(A)\n",
    "    F = Vh[-1].reshape(3, 3)\n",
    "\n",
    "    U, S, Vh = np.linalg.svd(F)\n",
    "    S[2] = 0\n",
    "    F = U @ np.diag(S) @ Vh\n",
    "    F = transform2.T @ F @ transform1\n",
    "    \n",
    "    return F\n",
    "\n",
    "def match_features(img1, img2):\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "    \n",
    "    matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    matches = sorted(matcher.match(des1, des2), key=lambda x: x.distance)[:200]\n",
    "    \n",
    "    points1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "    points2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "    \n",
    "    H, mask = cv2.findHomography(points1, points2, cv2.RANSAC, 5.0)\n",
    "    inliers = mask.ravel().astype(bool)\n",
    "    \n",
    "    return points1, points2, points1[inliers], points2[inliers]\n",
    "\n",
    "def plot_matches(img1, img2, points1, points2, inlier_points1=None, inlier_points2=None):\n",
    "    combined_img = np.hstack((img1, img2))\n",
    "    points2_shifted = points2.copy()\n",
    "    points2_shifted[:, 0] += img1.shape[1]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(combined_img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    for pt1, pt2 in zip(points1, points2_shifted):\n",
    "        plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]], 'y-', linewidth=0.5)\n",
    "\n",
    "    if inlier_points1 is not None and inlier_points2 is not None:\n",
    "        inlier_points2_shifted = inlier_points2.copy()\n",
    "        inlier_points2_shifted[:, 0] += img1.shape[1]\n",
    "        \n",
    "        for pt1, pt2 in zip(inlier_points1, inlier_points2_shifted):\n",
    "            plt.scatter([pt1[0], pt2[0]], [pt1[1], pt2[1]], c='cyan', s=40, edgecolors='blue')\n",
    "            plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]], 'r-', linewidth=1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_epipolar_line(img, line, point, height, width):\n",
    "    if abs(line[1]) < 1e-10:\n",
    "        return\n",
    "        \n",
    "    y0 = -line[2] / line[1]\n",
    "    yw = -(line[2] + line[0] * width) / line[1]\n",
    "    \n",
    "    x0 = -line[2] / line[0] if abs(line[0]) > 1e-10 else width\n",
    "    xh = -(line[2] + line[1] * height) / line[0] if abs(line[0]) > 1e-10 else width\n",
    "    \n",
    "    points = []\n",
    "    if 0 <= y0 <= height: points.append([0, y0])\n",
    "    if 0 <= yw <= height: points.append([width, yw])\n",
    "    if 0 <= x0 <= width: points.append([x0, 0])\n",
    "    if 0 <= xh <= width: points.append([xh, height])\n",
    "    \n",
    "    if len(points) >= 2:\n",
    "        points.sort(key=lambda x: x[0])\n",
    "        plt.plot([points[0][0], points[1][0]], [points[0][1], points[1][1]], 'r-', alpha=0.5)\n",
    "    \n",
    "    if 0 <= point[0] < width and 0 <= point[1] < height:\n",
    "        plt.scatter(point[0], point[1], c='cyan', s=40, edgecolors='blue')\n",
    "\n",
    "def visualize_epipolar_geometry(img1, img2, points1, points2, F):\n",
    "    epipole1 = null_space(F)[:, 0]\n",
    "    epipole2 = null_space(F.T)[:, 0]\n",
    "    \n",
    "    epipole1 = epipole1 / epipole1[2]\n",
    "    epipole2 = epipole2 / epipole2[2]\n",
    "    \n",
    "    lines1 = (F.T @ np.column_stack((points2, np.ones(points2.shape[0]))).T).T\n",
    "    lines2 = (F @ np.column_stack((points1, np.ones(points1.shape[0]))).T).T\n",
    "    \n",
    "    height, width = img1.shape[:2]\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(img1, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    for line, point in zip(lines1, points1):\n",
    "        plot_epipolar_line(img1, line, point, height, width)\n",
    "    if 0 <= epipole1[0] <= width and 0 <= epipole1[1] <= height:\n",
    "        plt.plot(epipole1[0], epipole1[1], 'rx', markersize=10)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(img2, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    for line, point in zip(lines2, points2):\n",
    "        plot_epipolar_line(img2, line, point, height, width)\n",
    "    if 0 <= epipole2[0] <= width and 0 <= epipole2[1] <= height:\n",
    "        plt.plot(epipole2[0], epipole2[1], 'rx', markersize=10)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def get_camera_poses(F, K):\n",
    "    E = K.T @ F @ K\n",
    "    U, _, Vh = np.linalg.svd(E)\n",
    "    W = np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]])\n",
    "    \n",
    "    R1 = U @ W @ Vh\n",
    "    R2 = U @ W.T @ Vh\n",
    "    t = U[:, 2]\n",
    "    \n",
    "    if np.linalg.det(R1) < 0: R1 = -R1\n",
    "    if np.linalg.det(R2) < 0: R2 = -R2\n",
    "        \n",
    "    return [(R1, t), (R1, -t), (R2, t), (R2, -t)]\n",
    "\n",
    "def triangulate_points(P1, P2, points1, points2):\n",
    "    points1_h = np.column_stack((points1, np.ones(len(points1))))\n",
    "    points2_h = np.column_stack((points2, np.ones(len(points2))))\n",
    "    points_3d = []\n",
    "    \n",
    "    for x1, x2 in zip(points1_h, points2_h):\n",
    "        A = np.zeros((4, 4))\n",
    "        A[0] = x1[0] * P1[2] - P1[0]\n",
    "        A[1] = x1[1] * P1[2] - P1[1]\n",
    "        A[2] = x2[0] * P2[2] - P2[0]\n",
    "        A[3] = x2[1] * P2[2] - P2[1]\n",
    "        \n",
    "        _, _, Vh = np.linalg.svd(A)\n",
    "        point = Vh[-1]\n",
    "        points_3d.append(point[:3] / point[3])\n",
    "    \n",
    "    return np.array(points_3d)\n",
    "\n",
    "def select_best_pose(poses, K, points1, points2):\n",
    "    P1 = K @ np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "    max_valid_points = 0\n",
    "    best_result = None\n",
    "    \n",
    "    for R, t in poses:\n",
    "        P2 = K @ np.hstack((R, t.reshape(3, 1)))\n",
    "        points_3d = triangulate_points(P1, P2, points1, points2)\n",
    "        \n",
    "        projected = (P2 @ np.column_stack((points_3d, np.ones(len(points_3d)))).T).T\n",
    "        valid_points = np.sum(projected[:, 2] > 0)\n",
    "        \n",
    "        if valid_points > max_valid_points:\n",
    "            max_valid_points = valid_points\n",
    "            best_result = (R, t, points_3d)\n",
    "            \n",
    "    return best_result\n",
    "\n",
    "def visualize_3d_scene(points_3d, R, t):\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(points_3d)\n",
    "\n",
    "    camera1 = o3d.geometry.PointCloud()\n",
    "    camera2 = o3d.geometry.PointCloud()\n",
    "    camera1.points = o3d.utility.Vector3dVector([[0, 0, 0]])\n",
    "    camera2.points = o3d.utility.Vector3dVector([-R.T @ t])\n",
    "\n",
    "    camera1.paint_uniform_color([1, 0, 0])\n",
    "    camera2.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "    arrow1 = o3d.geometry.TriangleMesh.create_arrow(0.05, 0.1, 0.02, 0.3)\n",
    "    arrow2 = o3d.geometry.TriangleMesh.create_arrow(0.05, 0.1, 0.02, 0.3)\n",
    "\n",
    "    arrow1.paint_uniform_color([1, 0, 0])\n",
    "    arrow2.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "    arrow2.translate(-R.T @ t)\n",
    "    arrow2.rotate(R, center=-R.T @ t)\n",
    "\n",
    "    o3d.visualization.draw_geometries([point_cloud, camera1, camera2, arrow1, arrow2])\n",
    "\n",
    "img1 = cv2.imread(\"data_Q2/first_img.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(\"data_Q2/second_img.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "intrinsics = np.loadtxt(\"data_Q2/intrinsics.txt\")\n",
    "\n",
    "points1, points2, inlier_points1, inlier_points2 = match_features(img1, img2)\n",
    "F = compute_fundamental_matrix(inlier_points1, inlier_points2)\n",
    "\n",
    "plot_matches(img1, img2, points1, points2, inlier_points1, inlier_points2)\n",
    "visualize_epipolar_geometry(img1, img2, inlier_points1, inlier_points2, F)\n",
    "\n",
    "poses = get_camera_poses(F, intrinsics)\n",
    "R, t, points_3d = select_best_pose(poses, intrinsics, inlier_points1, inlier_points2)\n",
    "visualize_3d_scene(points_3d, R, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
