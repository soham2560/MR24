{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roll number: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Instructions\n",
    " * Fill in the roll-number in the cell above.\n",
    " * Code must be submitted in Python in jupyter notebooks. We highly recommend using anaconda/miniconda distribution or at the minimum, virtual environments for this assignment.\n",
    " * All the code and result files should be uploaded in the github classroom.\n",
    " * For this assignment, you will be using Open3D  extensively. Refer to [Open3D](http://www.open3d.org/docs/release/) documentation.\n",
    " *  Most of the questions require you to **code your own functions** unless there is a need to call in the abilities of the mentioned libraries, such as Visualisation from Open3D. Make sure your code is modular since you will be reusing them for future assignments. All the functions related to transformation matrices, quaternions, and 3D projection are expected to be coded by you.\n",
    " *  All the representations are expected to be in a right-hand coordinate system.\n",
    "<!--  * Answer to the descriptive questions should be answered in your own words. Copy-paste answers will lead to penalty. -->\n",
    " * You could split the Jupyter Notebook cells where TODO is written, but please try to avoid splitting/changing the structure of other cells.\n",
    " * All the visualization should be done inside the notebook unless specified otherwise.\n",
    " * Plagiarism will lead to heavy penalty.\n",
    " * Commit the notebooks in the repo and any other results files under the result folder in the GitHub Classroom repo. \n",
    " * Commits past the deadline will not be considered.\n",
    " * This is a group assignment. Discussions are encouraged but any sharing of code among different teams will be penalized. \n",
    "\n",
    "### Instructions for group formation\n",
    " * We have circulated google sheet in moodle to fill in team members. Please finalize the teams formation by 18th Aug (tentative deadline). Same teams will be working towards project and other 2 Assignments as well. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Transformations and Projections on Autonomous Driving Dataset (20 Points)\n",
    "\n",
    "In this question, you will work with real world autonomous driving dataset (sequence in Waymo dataset). The dataset has LiDAR point clouds, images. You are required to demonstrate: \n",
    "\n",
    "**I. Various transformations of rotation matrices as described in below tasks.**\n",
    "\n",
    "**II. Visualization as a result of above transformations in Open3D**\n",
    "\n",
    "## Given data:\n",
    "\n",
    "1.) `LiDAR Point Clouds` : Stored at each timestep in the folder `lidar`. The point clouds are provided in the ego frame attached to lidar sensor (vehicle's reference frame).\n",
    "\n",
    "2.) `Images` : Stored at each timestep in the folder `images`. \n",
    "\n",
    "**Naming Convention** : {timestep}_{cam_no}.jpg where timestep is specified in 3 digits and cam_no : [0, 1, 2] indicates centre, left and right camera respectively.\n",
    "\n",
    "3.) `Camera-to-Ego Transformations`: Stored in the folder `cam2ego`, which converts points from each camera's reference frame to the vehicle's (or ego) reference frame.\n",
    "\n",
    "4.) `Ego-to-World Transformations`: Stored in the folder `ego2world`, which converts points from the vehicle's reference frame to the world frame W.\n",
    "\n",
    "5.) `Camera Intrinsics`: Stored in the folder `intrinsics` provided for 3 cameras.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to read lidar data and camera instrinsics are provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to read instrinsic matrix\n",
    "\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "def read_intrinsic(timestep):\n",
    "    intrinsic = np.loadtxt(f\"sample_intrinsic_{timestep}.txt\")\n",
    "    fx, fy, cx, cy = intrinsic[0], intrinsic[1], intrinsic[2], intrinsic[3]\n",
    "    intrinsic_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "\n",
    "read_intrinsic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165454, 3)\n"
     ]
    }
   ],
   "source": [
    "# Helper function to read lidar data at timestep 0 (same logic to read lidars at all remaining timesteps)\n",
    "\n",
    "lidar_data = np.memmap('sample_lidar_data_000.bin',\n",
    "                dtype=np.float32,\n",
    "                mode=\"r\",\n",
    "            ).reshape(-1, 14)   # (165454, 14)\n",
    "\n",
    "lidar_origins = lidar_data[:, :3]\n",
    "lidar_points = lidar_data[:, 3:6]   # (165454, 3)\n",
    "lidar_ids = lidar_data[:, -1]   # (165454,)\n",
    "\n",
    "# Lidar points to be used \n",
    "print(lidar_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import gdown\n",
    "import shutil\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "def get_ego2world(timestep):\n",
    "    filename = f'ego2world/{timestep:03d}.txt'\n",
    "    transform = np.loadtxt(filename)\n",
    "    transform = transform.reshape(4, 4)\n",
    "    return transform\n",
    "\n",
    "def get_cam2ego(camera_id):\n",
    "    filename = f'cam2ego/{camera_id}.txt'\n",
    "    transform = np.loadtxt(filename)\n",
    "    transform = transform.reshape(4, 4)\n",
    "    return transform\n",
    "\n",
    "def get_intrinsics(camera_id):\n",
    "    filename = f'intrinsics/{camera_id}.txt'\n",
    "    intrinsics = np.loadtxt(filename)\n",
    "    intrinsic_matrix = np.array([[intrinsics[0], 0, intrinsics[2]], [0, intrinsics[1], intrinsics[3]], [0, 0, 1]])\n",
    "    return intrinsic_matrix\n",
    "\n",
    "def get_image_path(camera_id,timestep):\n",
    "    filename = f'images/{timestep:03d}_{camera_id}.jpg'\n",
    "    return filename\n",
    "\n",
    "def get_lidar_points(timestep):\n",
    "    filename = f'lidar/{timestep:03d}.bin'\n",
    "    lidar_data = np.memmap(filename,\n",
    "                dtype=np.float32,\n",
    "                mode=\"r\",\n",
    "            ).reshape(-1, 14)   # (165454, 14)\n",
    "    lidar_points = lidar_data[:, 3:6]   # (165454, 3)\n",
    "    return lidar_points\n",
    "\n",
    "def transform_vector(vector, transformation_matrix):\n",
    "    homogeneous_vector = np.append(vector, 1)\n",
    "    \n",
    "    transformed_vector = np.matmul(transformation_matrix, homogeneous_vector)\n",
    "\n",
    "    return transformed_vector[:3]/transformed_vector[3]\n",
    "\n",
    "def transform_point_cloud(point_cloud,transformation_matrix,downsample = 1.0):\n",
    "    transformed_point_cloud = np.zeros((int(point_cloud.shape[0] * downsample), 3))\n",
    "    for i in range(int(point_cloud.shape[0] * downsample)):\n",
    "        transformed_point_cloud[i] = transform_vector(point_cloud[int(i/downsample)],transformation_matrix)\n",
    "    return transformed_point_cloud\n",
    "\n",
    "def invert_transform(matrix):\n",
    "    rotation_matrix = matrix[:3, :3]\n",
    "    translation_vector = matrix[:3, 3]\n",
    "\n",
    "    inv_rotation_matrix = np.transpose(rotation_matrix)\n",
    "    inv_translation_vector = -np.matmul(inv_rotation_matrix, translation_vector)\n",
    "\n",
    "    inv_matrix = np.eye(4)\n",
    "    inv_matrix[:3, :3] = inv_rotation_matrix\n",
    "    inv_matrix[:3, 3] = inv_translation_vector\n",
    "    \n",
    "    return inv_matrix\n",
    "\n",
    "def concatenate_point_clouds(func_get_single_pcl,func_get_associated_transform,start_timestep,stop_timestep,downsample = 1.0):\n",
    "    concatenated_point_cloud = np.empty((0, 3))\n",
    "    for i in range(start_timestep, stop_timestep + 1):\n",
    "        concatenated_point_cloud = np.vstack([concatenated_point_cloud,\n",
    "            transform_point_cloud(func_get_single_pcl(i),func_get_associated_transform(i),downsample)]\n",
    "        )\n",
    "    return concatenated_point_cloud\n",
    "\n",
    "def visualize_points(lidar_points):\n",
    "    # Create Open3D point cloud object from numpy array\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(lidar_points)\n",
    "    o3d.visualization.draw_geometries([point_cloud])\n",
    "\n",
    "def alpha_rot(alpha):\n",
    "    return np.array([\n",
    "        [   1.0,    0.0,                0.0             ],\n",
    "        [   0.0,    math.cos(alpha),   -math.sin(alpha) ],\n",
    "        [   0.0,    math.sin(alpha),    math.cos(alpha) ]\n",
    "        ],dtype=np.float32)\n",
    "\n",
    "def beta_rot(beta):\n",
    "    return np.array([\n",
    "        [   math.cos(beta),     0.0,    math.sin(beta)  ],\n",
    "        [   0.0,                1.0,    0.0             ],\n",
    "        [  -math.sin(beta),     0.0,    math.cos(beta)  ]\n",
    "        ],dtype=np.float32)\n",
    "\n",
    "def gamma_rot(gamma):\n",
    "    return np.array([\n",
    "        [   math.cos(gamma),   -math.sin(gamma),    0.0 ],\n",
    "        [   math.sin(gamma),    math.cos(gamma),    0.0 ],\n",
    "        [   0.0,                0.0,                1.0 ]\n",
    "        ],dtype=np.float32)\n",
    "    \n",
    "def map_value_to_color(value, min_val, max_val, color1, color2):\n",
    "    normalized_value = (value - min_val) / (max_val - min_val)\n",
    "    color = (1 - normalized_value) * color1 + normalized_value * color2\n",
    "    return color.astype(np.uint8)\n",
    "\n",
    "def skew(vector):\n",
    "    return np.array([\n",
    "        [ 0,         -vector[2],  vector[1]  ],\n",
    "        [ vector[2],  0,         -vector[0]  ],\n",
    "        [-vector[1],  vector[0],  0          ]])\n",
    "\n",
    "def prepare_data():\n",
    "    # List of expected extracted folders\n",
    "    expected_folders = [\"cam2ego\", \"ego2world\", \"images\", \"intrinsics\", \"lidar\"]\n",
    "\n",
    "    # Flag to check if any folder is missing or empty\n",
    "    should_download = False\n",
    "\n",
    "    # Check if all folders exist and are not empty\n",
    "    for folder in expected_folders:\n",
    "        if not os.path.exists(folder) or not os.listdir(folder):\n",
    "            should_download = True\n",
    "            break\n",
    "\n",
    "    if should_download:\n",
    "        # Download the zip file using gdown\n",
    "        gdown.download(id=\"1HKQ6UGFlgkRpIZzZWs6GkVtXdCGKzT3Z\", output=\"data.zip\", quiet=False)\n",
    "\n",
    "        # Extract the contents of the zip file into a specific directory\n",
    "        with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "\n",
    "        # Define the extracted folder\n",
    "        extracted_folder = \"080_MR\"\n",
    "\n",
    "        # Move the contents of the extracted folder to the current directory\n",
    "        for item in os.listdir(extracted_folder):\n",
    "            s = os.path.join(extracted_folder, item)\n",
    "            d = os.path.join(\".\", item)\n",
    "            if os.path.isdir(s):\n",
    "                shutil.move(s, d)\n",
    "            else:\n",
    "                shutil.move(s, d)\n",
    "\n",
    "        # Delete the now empty extracted folder\n",
    "        os.rmdir(extracted_folder)\n",
    "\n",
    "        # Delete the downloaded zip file if no longer needed\n",
    "        os.remove(\"data.zip\")\n",
    "        # Get Concatened PCl ready for tasks \n",
    "    else:\n",
    "        print(\"All folders already exist and are not empty. Skipping download and extraction.\")\n",
    "    global conc_pcl\n",
    "    if not os.path.exists(\"conc_pcl.npy\"):\n",
    "        conc_pcl = concatenate_point_clouds(get_lidar_points, get_ego2world, 0, 197)\n",
    "        np.save('conc_pcl',conc_pcl)\n",
    "    else:\n",
    "        print(\"conc_pcl is already populated. Skipping concatenation.\")\n",
    "    conc_pcl = np.load('conc_pcl.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All folders already exist and are not empty. Skipping download and extraction.\n",
      "conc_pcl is already populated. Skipping concatenation.\n"
     ]
    }
   ],
   "source": [
    "prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Note:** Even though Waymo dataset has 5 cameras, you are given the dataset corresponding to middle 3 cameras only. Please ignore other 2 cameras.\n",
    "\n",
    "## Notation for tasks:\n",
    "\n",
    "a.) `Global Reference Frame G`: Defined as the first ego frame (i.e., the translation vector of ego2world[0] is the origin of frame G in world frame W).\n",
    "\n",
    "World Frame W: A fixed world reference frame.\n",
    "\n",
    "b.) `Ego Frame`: Attached to the LiDAR and changes as the vehicle moves.\n",
    "\n",
    "c.) `Camera Frames`: Each of the 5 cameras has its own frame, which changes as the vehicle moves.\n",
    "\n",
    "Note: Axis directions of `Ego Frame` and `Camera Frames` are aligned with the Waymo Coordinate System (LiDAR) described below\n",
    "\n",
    "## Coordinate Systems:\n",
    "\n",
    "**OpenCV Coordinate System:** x right, y down, z front.\n",
    "\n",
    "**Waymo Coordinate System (LiDAR):** x front, y left, z up.\n",
    "\n",
    "\n",
    "![Waymo Setup](./waymo_setup.jpg \"Waymo Setup\")\n",
    "\n",
    "Link to dataset (one sequence) : https://drive.google.com/drive/folders/17YDx2Yn1KmPjmlaHsoFz4Jpa8zpgovO2?usp=drive_link\n",
    "\n",
    "If you want to try on other sequences as well, please refer to : https://waymo.com/open/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task 1`. Transformations of LiDAR Point Clouds (10 points)\n",
    "\n",
    "**Instructions:** \n",
    "\n",
    "Transform the LiDAR point clouds at all timesteps to the global reference frame G. Concatenate these transformed point clouds.\n",
    "    \n",
    "Visualization: Use Open3D to visualize the concatenated point cloud in the global reference frame G. Also, display the concatenation process at every timestep starting from first point cloud\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'o3d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##############################################################################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# TODO: TASK 1\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m##############################################################################\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m vis \u001b[38;5;241m=\u001b[39m \u001b[43mo3d\u001b[49m\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mVisualizer()\n\u001b[1;32m      5\u001b[0m vis\u001b[38;5;241m.\u001b[39mcreate_window()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create Open3D point cloud object from numpy array\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'o3d' is not defined"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: TASK 1\n",
    "##############################################################################\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "# Create Open3D point cloud object from numpy array\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "point_cloud.points = o3d.utility.Vector3dVector(transform_point_cloud(get_lidar_points(0),get_ego2world(0),0.1))\n",
    "vis.add_geometry(point_cloud)\n",
    "\n",
    "try:\n",
    "    for i in range(197):\n",
    "        point_cloud.points.extend(o3d.utility.Vector3dVector(transform_point_cloud(get_lidar_points(i+1), get_ego2world(i+1), 0.1)))\n",
    "        vis.update_geometry(point_cloud)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Visualization interrupted. Closing the window...\")\n",
    "\n",
    "# Safe exiting on button press or keyboard interrupt\n",
    "while True:\n",
    "    try:\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Visualization interrupted. Closing the window...\")\n",
    "        break\n",
    "\n",
    "# Destroy the window properly\n",
    "vis.destroy_window()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task 2`. Projecting LiDAR Point Clouds onto images (10 points)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Transform the concatenated point cloud from task 1 to the frame of each of the 3 cameras at timesteps `0, 20, and 55`. \n",
    "    \n",
    "Project these transformed point clouds onto the respective camera frames using the provided camera intrinsics. Concatenated point cloud would be very dense, so randomly select arbitrary number of points for better visualization. \n",
    "\n",
    "**Projected image pixel x : K * X_3d where X_3d is the 3d point in camera frame.**\n",
    "    \n",
    "Visualization: Overlay the projected points onto the camera images and visualize them.\n",
    "\n",
    "**For example:** Overlayed concatenated point cloud on camera `000_0.png` and `030_2.png` are shown below\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"./projected_000_0.png\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "<td> <img src=\"./projected_030_2.png\" alt=\"Drawing\" style=\"width: 750px;\"/> </td>\n",
    "</tr></table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: TASK 2\n",
    "##############################################################################\n",
    "def task2(cam,timesteps,resolution = 1.0):    \n",
    "    cam2cv = np.array([\n",
    "        [0.0,-1.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, -1.0, 0.0],\n",
    "        [1.0, 0.0,0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 1.0]\n",
    "    ], dtype=np.float32)\n",
    "    intrinsics = get_intrinsics(cam)\n",
    "    ego2cam = invert_transform(get_cam2ego(cam))\n",
    "    world2ego = invert_transform(get_ego2world(timesteps))\n",
    "\n",
    "    camera_frame_point_cloud = transform_point_cloud(conc_pcl,np.matmul(cam2cv,np.matmul(ego2cam,world2ego)),resolution)\n",
    "\n",
    "    pixels = np.empty((camera_frame_point_cloud.shape[0], 3))\n",
    "    for i in range(camera_frame_point_cloud.shape[0]):\n",
    "        pixels[i] = np.matmul(intrinsics,camera_frame_point_cloud[i])\n",
    "        pixels[i] /= pixels[i][2]\n",
    "    image = cv2.imread(get_image_path(cam,timesteps))\n",
    "    for i in range(pixels.shape[0]):\n",
    "        x, y = np.round(pixels[i][:2]).astype(int)\n",
    "        if 0 <= x < image.shape[1] and 0 <= y < image.shape[0]:\n",
    "            cv2.circle(image, (x, y), radius=2, color=(0, 255, 0), thickness=-1)\n",
    "\n",
    "    cv2.namedWindow('Resizable Window', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Resizable Window', 400, 300)\n",
    "    cv2.imshow('Resizable Window', image)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task2(0,0,0.01)\n",
    "task2(2,30,0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "### `Task 3`. Compute Depth Image from Projected Point Cloud in camera frame (5 points)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "Using the projected point clouds to camera frame from task 2, visualize the depth image by considering only the z-coordinate of the projected points in the camera frame.\n",
    "\n",
    "Visualization: Display the depth image for each of the 3 cameras at timesteps `0, 20, and 55` alongside the corresponding RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: TASK 3\n",
    "##############################################################################\n",
    "def task3(cam,timesteps,resolution = 1.0):\n",
    "    cam2cv = np.array([\n",
    "        [0.0,-1.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, -1.0, 0.0],\n",
    "        [1.0, 0.0,0.0, 0.0],\n",
    "        [0.0, 0.0, 0.0, 1.0]\n",
    "    ], dtype=np.float32)\n",
    "    intrinsics = get_intrinsics(cam)\n",
    "    ego2cam = invert_transform(get_cam2ego(cam))\n",
    "    world2ego = invert_transform(get_ego2world(timesteps))\n",
    "\n",
    "    camera_frame_point_cloud = transform_point_cloud(conc_pcl,np.matmul(cam2cv,np.matmul(ego2cam,world2ego)),resolution)\n",
    "\n",
    "    pixels = np.empty((camera_frame_point_cloud.shape[0], 3))\n",
    "    for i in range(camera_frame_point_cloud.shape[0]):\n",
    "        pixels[i] = np.matmul(intrinsics,camera_frame_point_cloud[i])\n",
    "        pixels[i] /= pixels[i][2]\n",
    "    image = np.ones((1280, 1920), dtype=np.uint8) * 255\n",
    "    for i in range(camera_frame_point_cloud.shape[0]):\n",
    "        x, y = np.round(pixels[i][:2]).astype(int)\n",
    "        if 0 <= y < image.shape[0] and 0 <= x < image.shape[1]:\n",
    "            if((image[y,x])>(camera_frame_point_cloud[i][2]*10.0)):\n",
    "                image[y,x] = (camera_frame_point_cloud[i][2]*10)\n",
    "\n",
    "    cv2.namedWindow('Resizable Window', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Resizable Window', 400, 300)\n",
    "    cv2.imshow('Resizable Window', image)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "task3(0,0,0.1)\n",
    "task3(1,0)\n",
    "task3(2,0)\n",
    "task3(0,20)\n",
    "task3(1,20)\n",
    "task3(2,20)\n",
    "task3(0,55)\n",
    "task3(1,55)\n",
    "task3(2,55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: You might be asked to show the above results for different timesteps and from one of the 3 cameras during evaluation/viva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Various Representations for Rotations and Gimbal lock (15 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Euler angles (2.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Write a function that returns a rotation matrix given the angles (ùõº, ùõΩ, ùõæ) = (2œÄ/5, œÄ/18, œÄ/6) in radians (X-Y-Z). Do not use inbuilt functions.\n",
    "\n",
    "b. Solve for angles using fsolve from scipy for three initializations of your choice and compare.\n",
    "$$M(\\alpha , \\beta ,\\gamma)=\\left[\\begin{array}{rrr}0.26200263 & -0.19674724 &  0.944799  \\\\0.21984631 &  0.96542533  & 0.14007684 \\\\\n",
    "   -0.93969262 & 0.17101007 & 0.29619813\\end{array}\\right] \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.1 (a)\n",
    "##############################################################################\n",
    "def euler2rm(alpha, beta, gamma):\n",
    "    return np.matmul(alpha_rot(alpha),np.matmul(beta_rot(beta),gamma_rot(gamma)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8528685  -0.49240386  0.17364818]\n",
      " [ 0.29753193  0.18504195 -0.93660784]\n",
      " [ 0.42905715  0.85046923  0.30432233]]\n"
     ]
    }
   ],
   "source": [
    "alpha = (2*math.pi)/5.0\n",
    "beta = (math.pi)/18.0\n",
    "gamma = (math.pi)/6.0\n",
    "print(euler2rm(alpha,beta,gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.55414196  1.31585304 -0.31566263]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_225429/3350393974.py:15: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last five Jacobian evaluations.\n",
      "  x = fsolve(f,[0,2,0],args = target_matrix)\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.1 (b)\n",
    "##############################################################################\n",
    "target_matrix = np.array([\n",
    "    [0.26200263, -0.19674724, 0.944799  ],\n",
    "    [0.21984631,  0.96542533, 0.14007684],\n",
    "    [-0.93969262, 0.17101007, 0.29619813],\n",
    "    ], dtype=np.float32)\n",
    "def f(euler,rm):\n",
    "    alpha,beta,gamma = euler\n",
    "    residual_matrix  = euler2rm(alpha,beta,gamma) - rm\n",
    "    return np.sum(np.abs(residual_matrix), axis=1)\n",
    "\n",
    "from scipy.optimize import fsolve\n",
    "x = fsolve(f,[0,2,0],args = target_matrix)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Equivalent angle‚Äìaxis representation (2.5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Write a function to convert equivalent angle‚Äìaxis representation (with a general axis and angle) to matrix form and vice versa. \\\n",
    "Try it for $\\theta = \\pi/6$ and axis $K= [1, 2, 3]^T $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -1.23205081  1.40192379]\n",
      " [ 1.76794919  1.40192379  0.30384758]\n",
      " [-0.59807621  1.30384758  2.07179677]]\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.2 \n",
    "##############################################################################\n",
    "\n",
    "def axisangle2rm(theta, axis):\n",
    "    return ((1 - np.cos(theta))*(np.matmul(np.atleast_2d(axis).transpose(),np.atleast_2d(axis)))) + (np.cos(theta)*np.eye(3)) + np.sin(theta)*skew(axis)\n",
    "\n",
    "print(axisangle2rm(np.pi/6,np.array([1,2,3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Gimbal lock (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an example where a Gimbal lock occurs and visualize the Gimbal lock on the given point cloud, data/toothless.ply. You have to show the above by animation (rotation along each axis one by one).\n",
    "\n",
    "**Hint:** \n",
    "Create 3 disks perpendicular to each other representing axes for local frame of object. Show that in certain configuration, due to use of Euler angles we can lose a degree of freedom. \n",
    "\n",
    "Use Open3D's non-blocking visualization and discretize the rotation to simulate the animation. For example, if you want to rotate by 20¬∞ around a particular axis, do so in increments of 5¬∞ 4 times to make it look like an animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load the point cloud\n",
    "pcd = o3d.io.read_point_cloud(\"toothless.ply\")\n",
    "\n",
    "# Create the axes disks\n",
    "def create_disk(radius=0.1, axis=[1, 0, 0], resolution=30):\n",
    "    return o3d.geometry.TriangleMesh.create_cylinder(radius=radius, height=0.01, resolution=resolution).rotate(\n",
    "        o3d.geometry.get_rotation_matrix_from_xyz(np.pi/2 * np.array(axis))\n",
    "    ).translate(np.array(axis) * radius)\n",
    "\n",
    "# Create the three disks representing the axes\n",
    "disk_x = create_disk(axis=[1, 0, 0])\n",
    "disk_y = create_disk(axis=[0, 1, 0])\n",
    "disk_z = create_disk(axis=[0, 0, 1])\n",
    "\n",
    "# Combine the disks with the point cloud\n",
    "geometry = [pcd, disk_x, disk_y, disk_z]\n",
    "\n",
    "# Create the visualizer\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "\n",
    "for geom in geometry:\n",
    "    vis.add_geometry(geom)\n",
    "\n",
    "# Function to apply rotation and simulate animation\n",
    "def rotate_and_visualize(vis, geometry, axis, angle_deg):\n",
    "    axis = np.array(axis)  # Convert the axis to a NumPy array\n",
    "    angle_rad = np.deg2rad(angle_deg)\n",
    "    R = o3d.geometry.get_rotation_matrix_from_xyz(axis * angle_rad)\n",
    "    for geom in geometry:\n",
    "        geom.rotate(R, center=(0, 0, 0))\n",
    "        vis.update_geometry(geom)  # Update each geometry object\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "\n",
    "# Simulate rotation animation\n",
    "angles = np.arange(0, 90 + 1, 1)  # Rotate 90 degrees in 5 degree steps\n",
    "\n",
    "for angle in angles:\n",
    "    rotate_and_visualize(vis, geometry, axis=[1, 0, 0], angle_deg=5)\n",
    "\n",
    "for angle in angles:\n",
    "    rotate_and_visualize(vis, geometry, axis=[0, 1, 0], angle_deg=5)\n",
    "\n",
    "for angle in angles:\n",
    "    rotate_and_visualize(vis, geometry, axis=[0, 0, 1], angle_deg=5)\n",
    "\n",
    "# Simulate Gimbal lock by rotating 90 degrees on Y and then 90 degrees on X\n",
    "rotate_and_visualize(vis, geometry, axis=[0, 1, 0], angle_deg=90)\n",
    "rotate_and_visualize(vis, geometry, axis=[1, 0, 0], angle_deg=90)\n",
    "\n",
    "# Here, rotating around the Z-axis won't change the orientation due to Gimbal lock\n",
    "    rotate_and_visualize(vis, geometry, axis=[0, 0, 1], angle_deg=5)\n",
    "\n",
    "# Get the current timestamp\n",
    "timestamp = datetime.now()\n",
    "\n",
    "# Run the loop for 10 seconds\n",
    "while (datetime.now() - timestamp) < timedelta(seconds=10):\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "\n",
    "vis.destroy_window()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4: Quaternions (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Convert a rotation matrix to quaternion and vice versa. Do not use inbuilt libraries for this question.\n",
    "\n",
    "b. Perform matrix multiplication of two 3√ó3 rotation matrices and perform the same transformation in the quaternion space. Verify if the final transformation obtained in both cases is the same.\n",
    "\n",
    "c. Try to interpolate any given model between two rotation matrices and visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.4 (a)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.4 (b)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Do tasks described in 2.4 (c)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Interpolation between transformations (15 points)\n",
    "\n",
    "Given 2 random transformation matrices, interpolate the given point cloud **toothless.ply** from `T2` to `T1` and visualize it.\n",
    "\n",
    "We will use the `generateTransformation()` function to generate a random Transformation matrix. You can write your own `generateTransformation()` function for testing, but we will replace it with our own so make sure that your code works for general cases.\n",
    "\n",
    "Ensure that your visualization shows the starting and ending configurations during interpolation.\n",
    "\n",
    "Your final output should look something like this:\n",
    "![Visualization](./out.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTransformation():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = generateTransformation()\n",
    "T2 = generateTransformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Implement the above question using spherical linear interpolation (slerp)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References: https://en.wikipedia.org/wiki/Slerp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makeagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
