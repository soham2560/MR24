{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roll number: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Instructions\n",
    " * Fill in the roll-number in the cell above.\n",
    " * Code must be submitted in Python in jupyter notebooks. We highly recommend using anaconda/miniconda distribution or at the minimum, virtual environments for this assignment.\n",
    " * All the code and result files should be uploaded in the github classroom.\n",
    " *  Most of the questions require you to **code your own functions** unless there is a need to call in the abilities of the mentioned libraries, such as Visualisation from Open3D. Make sure your code is modular since you will be reusing them for future assignments. All the functions related to transformation matrices, quaternions, and 3D projection are expected to be coded by you.\n",
    " *  All the representations are expected to be in a right-hand coordinate system.\n",
    "<!--  * Answer to the descriptive questions should be answered in your own words. Copy-paste answers will lead to penalty. -->\n",
    " * You could split the Jupyter Notebook cells where TODO is written, but please try to avoid splitting/changing the structure of other cells.\n",
    " * All the visualization should be done inside the notebook unless specified otherwise.\n",
    " * Plagiarism will lead to heavy penalty.\n",
    " * Commit the notebooks in the repo and any other results files under the result folder in the GitHub Classroom repo. \n",
    " * This is a group assignment. Discussions are encouraged but any sharing of code among different teams will be penalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: ICP with SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Perform Procrustes alignmenton two point clouds with (given) known correspondences. (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let X be your point cloud observed from the initial pose P1. You then transform it to a new pose P2. Now you wish to apply ICP to recover transformation between (X & P1) and (X & P2).\n",
    "\n",
    "Use toothless.ply point cloud and perform the alignment between the two point clouds using procrustes alignment. Your task is to write a function that takes two point clouds as input wherein the corresponding points between the two point clouds are located at the same index and returns the transformation matrix between them. Compute the alignment error after aligning the two point clouds.\n",
    "\n",
    "<b>Use root mean squared error (RSME) as the alignment error metric.</b>\n",
    "\n",
    "Make sure your code is modular as we will use this function in the next sub-part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use our own getTransform function to generate the 4x4 Transformation matrix to transform the pointcloud to P2, so make sure your code works for any general Transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_rot(alpha):\n",
    "    return np.array([\n",
    "        [   1.0,    0.0,                0.0             ],\n",
    "        [   0.0,    math.cos(alpha),   -math.sin(alpha) ],\n",
    "        [   0.0,    math.sin(alpha),    math.cos(alpha) ]\n",
    "        ],dtype=np.float32)\n",
    "\n",
    "def beta_rot(beta):\n",
    "    return np.array([\n",
    "        [   math.cos(beta),     0.0,    math.sin(beta)  ],\n",
    "        [   0.0,                1.0,    0.0             ],\n",
    "        [  -math.sin(beta),     0.0,    math.cos(beta)  ]\n",
    "        ],dtype=np.float32)\n",
    "\n",
    "def gamma_rot(gamma):\n",
    "    return np.array([\n",
    "        [   math.cos(gamma),   -math.sin(gamma),    0.0 ],\n",
    "        [   math.sin(gamma),    math.cos(gamma),    0.0 ],\n",
    "        [   0.0,                0.0,                1.0 ]\n",
    "        ],dtype=np.float32)\\\n",
    "        \n",
    "def euler2rm(alpha, beta, gamma):\n",
    "    return np.matmul(alpha_rot(alpha),np.matmul(beta_rot(beta),gamma_rot(gamma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centre_of_mass_func(points):\n",
    "    return np.mean(points, axis=0)\n",
    "\n",
    "def error(target, predicted, transform_matrix):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransform():\n",
    "    # Random rotation matrices for X, Y, and Z axes\n",
    "    R = euler2rm(np.random.uniform(0, 2 * np.pi),\n",
    "                 np.random.uniform(0, 2 * np.pi),\n",
    "                 np.random.uniform(0, 2 * np.pi))\n",
    "    \n",
    "    # Random translation matrix\n",
    "    T = np.array([\n",
    "        [np.random.uniform(-500, 500)],\n",
    "        [np.random.uniform(-500, 500)],\n",
    "        [np.random.uniform(-500, 500)]\n",
    "    ])\n",
    "    \n",
    "    transformation_matrix = np.eye(4)\n",
    "    transformation_matrix[:3, :3] = R\n",
    "    transformation_matrix[:3, 3] = T.flatten()\n",
    "    \n",
    "    return transformation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.28572121e-01  2.09519267e-01  9.69314635e-01  1.78174213e+02]\n",
      " [-1.17578976e-01 -9.67312276e-01  2.24682376e-01  3.74621347e+02]\n",
      " [ 9.84705269e-01 -1.42858908e-01 -9.97343212e-02 -3.11425021e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "518.729818034171"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def icp_known_correspondences(T, pcd):\n",
    "    #### YOUR CODE HERE ####\n",
    "    # P1 point cloud (initial pose)\n",
    "    P1 = pcd\n",
    "\n",
    "    # P2 point cloud (Transform pose)\n",
    "    print(T)\n",
    "    P2 = pcd.transform(T)\n",
    "\n",
    "    # Numpy array convertion\n",
    "    P1_np = np.asarray(P1.points)\n",
    "    P2_np = np.asarray(P2.points)\n",
    "\n",
    "    # Calculate centre of Mass for P1, P2\n",
    "    centre_mass_P1 = centre_of_mass_func(P1_np)\n",
    "    centre_mass_P2 = centre_of_mass_func(P2_np)\n",
    "\n",
    "    # Normalisation of P1 and P2\n",
    "    P1_norm = P1_np - centre_mass_P1\n",
    "    P2_norm = P2_np - centre_mass_P2\n",
    "\n",
    "    # Cross-Covariance Matrix\n",
    "    W = P1_norm.T @ P2_norm\n",
    "\n",
    "    # SVD\n",
    "    U,D,Vt = np.linalg.svd(W)\n",
    "\n",
    "    # Rotational Matrix and Translation Matrix\n",
    "    R = U @ Vt\n",
    "    t = centre_mass_P1 - (R @ centre_mass_P2)\n",
    "    t = np.expand_dims(t, axis=-1)\n",
    "\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = R\n",
    "    T[:3, 3:] = t\n",
    "\n",
    "    return T \n",
    "\n",
    "# CODE \n",
    "T = getTransform()\n",
    "point_cloud_path = \"data/toothless.ply\"\n",
    "pcd = o3d.io.read_point_cloud(point_cloud_path)\n",
    "voxel_size = 5.0\n",
    "downsampled_pcd = pcd.voxel_down_sample(voxel_size=voxel_size)\n",
    "transform_matrix = icp_known_correspondences(T, downsampled_pcd)\n",
    "alignment_error = np.linalg.norm(transform_matrix - T)\n",
    "alignment_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implement ICP algorithm with unknown correspondences. (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to write a function that implements ICP and takes two point clouds as input wherein the correspondances are unknown. Visualize the pointclouds and plot their individual coordinate frames as you perform ICP over them. Compute the alignment error in each iteration. \n",
    "\n",
    "Refer to Shubodh's notes to compute correspondences: https://saishubodh.notion.site/Mobile-Robotics-Navigating-from-Theory-to-Application-0b65a9c20edd4081978f4ffad917febb?p=a25686ce1a11409d838d47bcac43ab4b&pm=s#bb9aaf2e316b4db3b399df1742f0444c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = getTransform()\n",
    "Q = getTransform()\n",
    "\n",
    "def icp_unknown_correspondences(P, Q):\n",
    "    #### YOUR CODE HERE ####\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2: ICP with Lie Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Predict the Transformation matrix between 2 point clouds with known correspondences (15 Points)\n",
    "Perform the same task as 1.1 using Lie Group Optimization from scratch to predict the transformation between the 2 point clouds.\n",
    "\n",
    "Refer: https://saishubodh.notion.site/Mobile-Robotics-Navigating-from-Theory-to-Application-0b65a9c20edd4081978f4ffad917febb?p=ee55fe5689794693910ab7861bef067b&pm=s#7b82d84766a84b63b91d859579e4886b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = getTransform()\n",
    "def icp_with_lie(T):\n",
    "    #### YOUR CODE HERE ####\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 3: Pose Graph Optimization with G2O\n",
    "### Objective (5 Points)\n",
    "A robot is travelling in a oval trajectory. It is equipped with wheel odometry for odometry information and RGBD sensors for loop closure information. Due to noise in wheel odometry it generates a noisy estimate of the trajectory. Our task is to use loop closure pairs to correct the drift.\n",
    "\n",
    "We pose this problem as a pose graph optimization problem. In our graph, poses are the vertices and constraints are the edges. \n",
    "\n",
    "References:\n",
    "\n",
    "1.) Class notes: https://saishubodh.notion.site/G2O-Edge-Types-d9f9ff63c77c4ceeb84b1e49085004e3\n",
    "\n",
    "2.) Cyrill Stachniss lecture: https://www.youtube.com/watch?v=uHbRKvD8TWg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given: \n",
    "In practical scenarios, we'd obtain the following from our sensors after some post-processing:\n",
    "\n",
    "1. Initial position\n",
    "2. Odometry Contraints/Edges: This \"edge\" information tells us relative transformation between two nodes. These two nodes are consecutive in the case of Odometry but not in the case of Loop Closure (next point).\n",
    "3. Loop Closure Contraints/Edges: Remember that while optimizing, you have another kind of \"anchor\" edge as you've seen in 1. solved example.\n",
    "\n",
    "You have been given a text file named `edges.txt` (in `data/`) which has all the above 3 and it follows G2O's format (as explained in class, [link here](https://saishubodh.notion.site/G2O-Edge-Types-d9f9ff63c77c4ceeb84b1e49085004e3) ). The ground truth is `gt.txt`.\n",
    "\n",
    "Install g2o as mentioned in `g2o.ipynb` and optimise `edges.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evo (10 Points)\n",
    "We need a measure of how good the trajectory is. The error/loss used earlier doesn't tell us much about how the trajectory differs from the ground truth. Here, we try to do just this - compute error metrics. Rather than computing these from scratch, we will just Evo - https://github.com/MichaelGrupp/evo/.\n",
    "\n",
    "Look at the absolute pose error (APE) and relative pose error (RPE). What do they capture and how are they calculated (descriptive answer)? How do these metrics differ in methodology? Can we determine if the error is more along the x/y axis?\n",
    "\n",
    "Answer the above questions and report errors for the obtained trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mapfree",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
